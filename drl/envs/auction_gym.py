import gym
import numpy as np
from gym import spaces
from typing import Optional, Dict, Any, List

from drl.envs.sim_wrapper import SimulationEnv

class AuctionGymEnv(gym.Env):
    """Enhanced Gym environment for traffic intersection auction system"""
    
    metadata = {'render.modes': ['human', 'rgb_array']}

    def __init__(self, sim_cfg: Dict = None):
        super().__init__()
        
        self.sim_cfg = sim_cfg or {}
        self.sim = SimulationEnv(self.sim_cfg)
        
        # Define observation space - ç¡®ä¿ä¸sim_wrapperä¸€è‡´
        obs_dim = self.sim.observation_dim()
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(obs_dim,), dtype=np.float32
        )
        
        # æ‰©å±•åŠ¨ä½œç©ºé—´ - åŒ…å«æ‰€æœ‰14ä¸ªå¯è®­ç»ƒå‚æ•°
        self.action_space = spaces.Box(
            low=np.array([
                0.1,  # bid_scale
                0.5,  # eta_weight
                0.0,  # speed_weight
                0.0,  # congestion_sensitivity
                0.0,  # platoon_bonus
                0.0,  # junction_penalty
                0.0,  # fairness_factor
                1.0,  # urgency_threshold
                0.0,  # proximity_bonus_weight
                -30.0, # speed_diff_modifier
                -2.0, # follow_distance_modifier
                0.0,  # ignore_vehicles_go
                0.0,  # ignore_vehicles_wait
                0.0   # avg_ignore_vehicles_platoon (leader+follower)/2
            ], dtype=np.float32),
            high=np.array([
                5.0,  # bid_scale
                3.0,  # eta_weight
                1.0,  # speed_weight
                1.0,  # congestion_sensitivity
                2.0,  # platoon_bonus
                1.0,  # junction_penalty
                0.5,  # fairness_factor
                10.0, # urgency_threshold
                3.0,  # proximity_bonus_weight
                30.0, # speed_diff_modifier
                3.0,  # follow_distance_modifier
                100.0, # ignore_vehicles_go
                50.0, # ignore_vehicles_wait
                100.0 # avg_ignore_vehicles_platoon
            ], dtype=np.float32),
            shape=(14,), 
            dtype=np.float32
        )
        
        self.current_obs = None
        self.render_mode = None
        
        print("ğŸ® å®Œå…¨æ‰©å±•çš„Auction Gym Environmentåˆå§‹åŒ–")
        print(f"   è§‚å¯Ÿç©ºé—´: {self.observation_space.shape} (ç¡®ä¿209ç»´)")
        print(f"   åŠ¨ä½œç©ºé—´: {self.action_space.shape} (14ä¸ªå¯è®­ç»ƒå‚æ•°)")

    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> np.ndarray:
        """Reset environment"""
        super().reset(seed=seed)
        obs = self.sim.reset(seed=seed)
        self.current_obs = obs
        
        # éªŒè¯è§‚å¯Ÿç»´åº¦
        expected_shape = self.observation_space.shape[0]
        if obs.shape[0] != expected_shape:
            print(f"âš ï¸ Resetè§‚å¯Ÿç»´åº¦ä¸åŒ¹é…: æœŸæœ› {expected_shape}, å®é™… {obs.shape[0]}")
            # ç¡®ä¿ç»´åº¦æ­£ç¡®
            if obs.shape[0] < expected_shape:
                padding = np.zeros(expected_shape - obs.shape[0], dtype=np.float32)
                obs = np.concatenate([obs, padding])
            else:
                obs = obs[:expected_shape]
        
        return obs

    def step(self, action: np.ndarray) -> tuple:
        """Enhanced step with all trainable parameters"""
        # è§£ææ‰€æœ‰14ä¸ªå‚æ•°
        action_params = {
            'bid_scale': float(action[0]),
            'eta_weight': float(action[1]),
            'speed_weight': float(action[2]),
            'congestion_sensitivity': float(action[3]),
            'platoon_bonus': float(action[4]),
            'junction_penalty': float(action[5]),
            'fairness_factor': float(action[6]),
            'urgency_threshold': float(action[7]),
            'proximity_bonus_weight': float(action[8]),
            'speed_diff_modifier': float(action[9]),
            'follow_distance_modifier': float(action[10]),
            'ignore_vehicles_go': float(action[11]),
            'ignore_vehicles_wait': float(action[12]),
        }
        
        # å¤„ç†è½¦é˜Ÿçš„ignore_vehicleså‚æ•° (ä»å¹³å‡å€¼è®¡ç®—)
        avg_platoon_ignore = float(action[13])
        action_params['ignore_vehicles_platoon_leader'] = max(0.0, avg_platoon_ignore - 20.0)
        action_params['ignore_vehicles_platoon_follower'] = min(100.0, avg_platoon_ignore + 20.0)
        
        # æ›´æ–°ä»¿çœŸ
        obs, reward, done, info = self.sim.step_with_all_params(action_params)
        
        # éªŒè¯è§‚å¯Ÿç»´åº¦
        expected_shape = self.observation_space.shape[0]
        if obs.shape[0] != expected_shape:
            print(f"âš ï¸ Stepè§‚å¯Ÿç»´åº¦ä¸åŒ¹é…: æœŸæœ› {expected_shape}, å®é™… {obs.shape[0]}")
            # ç¡®ä¿ç»´åº¦æ­£ç¡®
            if obs.shape[0] < expected_shape:
                padding = np.zeros(expected_shape - obs.shape[0], dtype=np.float32)
                obs = np.concatenate([obs, padding])
            else:
                obs = obs[:expected_shape]
        
        self.current_obs = obs
        
        # å¢å¼ºä¿¡æ¯åŒ…å«æ‰€æœ‰å‚æ•°
        info.update({
            'action_params': action_params,
            'total_trainable_params': 14,
            'observation_shape': obs.shape[0]
        })
        
        return obs, float(reward), bool(done), info

    def render(self, mode: str = 'human') -> Optional[np.ndarray]:
        """Enhanced render with visualization options"""
        if mode == 'human':
            self._render_human()
        elif mode == 'rgb_array':
            return self._render_rgb_array()
        else:
            print(f"Unsupported render mode: {mode}")

    def _render_human(self):
        """Human-readable console rendering"""
        if hasattr(self.sim, 'metrics'):
            print(f"\nğŸ® Simulation State:")
            print(f"   Throughput: {self.sim.metrics['throughput']:.1f} vehicles/h")
            print(f"   Avg Acceleration: {self.sim.metrics['avg_acceleration']:.3f} m/sÂ²")
            print(f"   Collisions: {self.sim.metrics['collision_count']}")
            print(f"   Step: {self.sim.current_step}/{self.sim.max_steps}")
            
            # Policy information
            if hasattr(self.sim, 'bid_policy'):
                policy_stats = self.sim.bid_policy.get_policy_stats()
                print(f"   Bid Scale: {policy_stats.get('current_bid_scale', 0):.2f}")
                print(f"   Success Rate: {policy_stats.get('success_rate', 0):.1%}")

    def _render_rgb_array(self) -> np.ndarray:
        """Render as RGB array for video recording"""
        # This would require implementing a visual renderer
        # For now, return a placeholder
        return np.zeros((600, 800, 3), dtype=np.uint8)

    def close(self) -> None:
        """Close environment"""
        if hasattr(self, 'sim'):
            self.sim.close()
        print("ğŸ Enhanced Auction Gym Environment closed")

    def get_action_meanings(self) -> List[str]:
        """Get human-readable action descriptions for all 14 parameters"""
        return [
            "Bid Scale (0.1-5.0): æ€»ä½“å‡ºä»·ç¼©æ”¾å› å­",
            "ETA Weight (0.5-3.0): ETAåˆ°è¾¾æ—¶é—´æƒé‡", 
            "Speed Weight (0.0-1.0): è½¦è¾†é€Ÿåº¦æƒé‡",
            "Congestion Sensitivity (0.0-1.0): æ‹¥å µæ•æ„Ÿåº¦",
            "Platoon Bonus (0.0-2.0): è½¦é˜Ÿå¥–åŠ±ç³»æ•°",
            "Junction Penalty (0.0-1.0): è·¯å£ä½ç½®æƒ©ç½š",
            "Fairness Factor (0.0-0.5): å…¬å¹³æ€§è°ƒèŠ‚å› å­",
            "Urgency Threshold (1.0-10.0): ç´§æ€¥åº¦é˜ˆå€¼",
            "Proximity Bonus Weight (0.0-3.0): é‚»è¿‘æ€§å¥–åŠ±æƒé‡",
            "Speed Diff Modifier (-30 to +30): é€Ÿåº¦æ§åˆ¶ä¿®æ­£",
            "Follow Distance Modifier (-2 to +3): è·Ÿè½¦è·ç¦»ä¿®æ­£",
            "Ignore Vehicles Go (0-100): GOçŠ¶æ€ignore_vehicles%",
            "Ignore Vehicles Wait (0-50): WAITçŠ¶æ€ignore_vehicles%",
            "Avg Platoon Ignore Vehicles (0-100): è½¦é˜Ÿå¹³å‡ignore_vehicles%"
        ]

    def get_reward_info(self) -> Dict[str, str]:
        """Get information about reward components"""
        return {
            "throughput": "Vehicles successfully exiting intersection (+10 per vehicle)",
            "safety": "Collision avoidance (-100 per collision)",
            "efficiency": "Smooth acceleration patterns (+5 for low jerk)",
            "utilization": "Optimal intersection usage (+5 for good ratios)",
            "deadlock_penalty": "Deadlock avoidance (-50 per deadlock)",
            "step_penalty": "Encourage efficiency (-0.1 per step)"
        }

    def get_safety_metrics(self) -> Dict[str, Any]:
        """Get safety-related metrics"""
        if hasattr(self.sim, 'metrics'):
            return {
                'collision_count': self.sim.metrics.get('collision_count', 0),
                'deadlock_detections': getattr(self.sim, 'deadlock_detector', None).get_stats().get('deadlocks_detected', 0) if hasattr(self.sim, 'deadlock_detector') else 0,
                'safety_score': self._calculate_safety_score()
            }
        return {}

    def _calculate_safety_score(self) -> float:
        """Calculate overall safety score (0-1, higher is safer)"""
        if not hasattr(self.sim, 'metrics'):
            return 1.0
        
        collision_penalty = min(self.sim.metrics.get('collision_count', 0) * 0.1, 0.5)
        deadlock_penalty = 0.0
        
        if hasattr(self.sim, 'deadlock_detector'):
            deadlock_stats = self.sim.deadlock_detector.get_stats()
            deadlock_penalty = min(deadlock_stats.get('deadlocks_detected', 0) * 0.2, 0.3)
        
        safety_score = max(0.0, 1.0 - collision_penalty - deadlock_penalty)
        return safety_score
